# Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM

**Research Implementation for AICCSA 2025**

This repository contains the implementation for the paper **â€œBeyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLMâ€** (AICCSA 2025). The framework addresses the limits of traditional star-rating systems by using Large Language Models (LLMs) to capture nuanced feedback that numeric ratings often miss.

---

## ğŸ¯ Research Objectives

Star ratings alone rarely reflect the full story in review text. Our LLM-based framework bridges this gap by:

- **Extracting rich insights** from free-form reviews (aspects, sentiments, recommendations).
- **Capturing linguistic nuances** (hedging, sarcasm, contrastive praise/critique).
- **Providing actionable feedback** for product teams.
- **Enabling interactive analysis** through evidence-grounded Q&A.

---

## ğŸ§© Framework Overview

**Core Architecture.** A modular, hybrid multi-stage pipeline where each component runs independently or as an end-to-end system, combining traditional NLP baselines with advanced LLM techniques.

- **M1 â€” Discrepancy Analysis (Baseline):**  
  VADER-based sentiment â†’ mapped to 1â€“5 scale â†’ absolute difference vs. star rating.

- **M2 â€” ABSA + Recommendation Mining:**  
  Structured prompting with domain examples to extract **(aspect, sentiment, recommendation)** triples.

- **M3 â€” LLM-Enhanced Topic Modeling:**  
  BERTopic for clusters, with LLM-generated topic **labels** and **summaries**.

- **M4 â€” RAG-Based Conversational QA:**  
  Interactive, **evidence-backed** question answering over large review sets.

### Multi-LLM Architecture

- **openai** â€” primary for complex reasoning & structured outputs  
- **LLaMA-2** â€” open-source alternative for accessibility & comparison  
- **Mistral** â€” efficient, high-quality responses  
- **Unified Interface** â€” seamless provider switching via a standardized `ChatFn` interface

### Multi-Dataset Validation

- Evaluated on [AWARE](https://zenodo.org/records/5528481), 
  [Google Play](https://www.kaggle.com/datasets/prakharrathi25/google-play-store-reviews), 
  and [Spotify](https://www.kaggle.com/datasets/ashishkumarak/spotify-reviews-playstore-daily-update) review corpora
  - **Scalable** from small samples to **80K+** reviews with intelligent caching

---

## ğŸš€ Quick Start

### Installation

1) **Clone the repository**
2) **Install dependencies**
```bash
pip install -r requirements.txt
```
3) **Set environment variables**  
Create a `.env` in the project root:
```env
OPENAI_API_KEY=...
MISTRAL_API_KEY=...
LLAMA_API_KEY=...
```

---

## ğŸ“ Core Architecture (Project Structure)

```
aiccsa2025-llm-app-review-analysis/      # Repository root
â”‚
â”œâ”€â”€ app_reviews_pipeline/                # Main pipeline package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_config.py                    # Multi-provider LLM configuration
â”‚   â”œâ”€â”€ preprocessing.py                 # Data cleaning & preparation
â”‚   â”œâ”€â”€ prompt_optimize.py               # Automated prompt optimization
â”‚   â”œâ”€â”€ quality_judge.py                 # LLM output quality evaluation
â”‚   â”œâ”€â”€ run_pipeline.py                  # Pipeline orchestration (CLI)
â”‚   â”œâ”€â”€ user_selection.py                # Interactive CLI utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ M1_Discrepancy/
â”‚   â”‚   â”œâ”€â”€ discrepancy.py               # VADER sentiment & discrepancy detection
â”‚   â”‚   â”œâ”€â”€ discrepancy_plots.ipynb      # Notebook version
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ M2_Absa_recommendation/
â”‚   â”‚   â”œâ”€â”€ absa_recommendation.py       # ABSA pipeline
â”‚   â”‚   â”œâ”€â”€ absa_prompts.py              # ABSA prompts
â”‚   â”‚   â”œâ”€â”€ absa_LLM_helpers.py          # ABSA utilities
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ M3_Topic_modeling/
â”‚   â”‚   â”œâ”€â”€ topic_modeling.py            # BERTopic + LLM labeling
â”‚   â”‚   â”œâ”€â”€ topic_prompts.py             # Topic labeling prompts
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ M4_Rag_qa/
â”‚       â”œâ”€â”€ rag_qa.py                    # RAG-based Q&A
â”‚       â”œâ”€â”€ rag_prompt.py                # RAG prompts/templates
â”‚       â”œâ”€â”€ rag_qus_samples.txt          # Sample questions
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ AWARE_Comprehensive.csv
â”‚   â”‚   â”œâ”€â”€ spotify_reviews.csv
â”‚   â”‚   â””â”€â”€ google_play_reviews.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ aware_clean.csv
â”‚       â”œâ”€â”€ spotify_clean.csv
â”‚       â”œâ”€â”€ google_play_clean.csv
â”‚       â””â”€â”€ *_stats.json                 # Dataset statistics
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ absa/
â”‚   â”œâ”€â”€ discrepancy/
â”‚   â”œâ”€â”€ topic_modeling/
â”‚   â”œâ”€â”€ rag_cache/
â”‚   â””â”€â”€ prompt_dumps/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ® Usage Guide

### 1) ğŸ§¹ Data Preprocessing
Clean and normalize any dataset placed under `data/raw/`:
```bash
python app_reviews_pipeline/preprocessing.py   --dataset aware   # options: aware | spotify | google_play
```
**Output:** Clean CSVs in `data/processed/` + summary stats in `*_stats.json`.

**Interactive Flow (when using `run_pipeline.py`):**
1. Choose dataset (AWARE, Spotify, Google Play)  
2. Select LLM provider and model  
3. Set sample size or use full dataset

### 2) â–¶ï¸ Run the full pipeline (interactive)
```bash
python app_reviews_pipeline/run_pipeline.py
```

### 3) Run individual modules

- **M1 â€” Discrepancy Analysis**
  ```bash
  python -m app_reviews_pipeline.M1_Discrepancy.discrepancy     --input data/processed/aware_clean.csv     --outdir outputs/discrepancy
  ```

- **M2 â€” ABSA + Recommendation Mining**
  ```bash
  python -m app_reviews_pipeline.M2_Absa_recommendation.absa_recommendation     --input data/processed/spotify_clean.csv     --provider openai --model gpt-4o     --outdir outputs/absa
  ```

- **M3 â€” Topic Modeling (BERTopic + LLM labels)**
  ```bash
  python -m app_reviews_pipeline.M3_Topic_modeling.topic_modeling     --input data/processed/google_play_clean.csv     --nr-topics 20 --embedding-model all-mpnet-base-v2     --outdir outputs/topic_modeling
  ```

- **M4 â€” RAG QA**
  ```bash
  python -m app_reviews_pipeline.M4_Rag_qa.rag_qa     --input data/processed/spotify_clean.csv     --cache-dir outputs/rag_cache
  ```

---

## ğŸ“Š Outputs

- **`outputs/discrepancy/`** â€” per-review sentiment, mapped scores, and rating/text gaps  
- **`outputs/absa/`** â€” extracted triples `(aspect, sentiment, recommendation)`  
- **`outputs/topic_modeling/`** â€” cluster labels, summaries, visualizations  
- **`outputs/rag_cache/`** â€” embeddings and indices for QA  
- **`outputs/prompt_dumps/`** â€” auto-optimized prompts for reproducibility

---

## ğŸ§ª Research Impact

- **Academic Contribution:** LLM framework surpassing traditional baselines on nuanced opinion mining.  
- **Practical Value:** Actionable, feature-level insights beyond star ratings.  
- **Methodological Innovation:** Structured prompting with automated optimization.  
- **Scalability:** Robust from small samples to tens of thousands of reviews.

---

## ğŸ’¡ Notes

- Put your raw CSVs in `data/raw/`.  
- Preprocessing writes cleaned files to `data/processed/`.  
- Ensure `.env` is configured before running LLM-dependent modules.
