# Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM

**Research Implementation for AICCSA 2025**

This repository contains the complete implementation of research paper "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM" published in AICCSA 2025. 
The framework addresses the fundamental limitations of traditional star-rating systems by developing an advanced LLM-based approach to app review analysis that captures nuanced feedback often missed by numeric ratings alone.

## Research Objectives

Our research tackles the critical challenge that **star ratings fail to represent the nuanced feedback present in review texts**. While a 5-star rating appears positive, the accompanying text might reveal significant concerns about specific app features. Our LLM framework bridges this gap by:

- **Extracting Rich Insights**
- **Capturing Linguistic Nuances**
- **Providing Actionable Feedback**
- **Enabling Interactive Analysis**

## Framework Description:
Core Architecture: The framework is a modular, hybrid multi-stage pipeline where each component can operate independently or together. It combines traditional NLP baselines with advanced LLM techniques for comprehensive app review analysis.
- **M1 - Discrepancy Analysis (Baseline)** - Uses VADER sentiment analyzer to quantify misalignment between star ratings and review text sentiment by mapping VADER scores to 1-5 star scale and computing absolute differences.
- **M2 - ABSA with Recommendation Mining** - Employs structured prompting with domain-specific examples to extract aspect-sentiment-recommendation triples.
- **M3 - LLM-Enhanced Topic Modeling** - Combines BERTopic clustering with intelligent LLM-generated topic labels and summaries from keyword clusters.
- **M4 - RAG-Based Conversational QA**: Interactive evidence-backed conversational question answering over large review datasets.

### Multi-LLM Architecture
- **GPT-4/4o**: Primary model for complex reasoning and structured outputs
- **LLaMA 2 7B-chat**: Open-source alternative for accessibility and comparison
- **Mistral 7B**: High-performance model with efficient resource usage
- **Unified Interface**: Seamless switching between providers via standardized ChatFn interface


### Multi-Dataset Validation**: 
- Tested on AWARE, Google Play, and Spotify review corpora
- **Scalable Architecture**: Handles datasets from samples to 80K+ reviews with intelligent caching

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- API keys for your chosen LLM provider(s)

### Installation

1. **Clone the repository:**
2. **Install dependencies
3. **Set up environment variables:**
Create a `.env` file in the root directory with Poviders API (OPENAI_API_KEY, MISTRAL_API_KEY, LLAMA_API_KEY)

## ğŸ“ Project Structure

aiccsa2025-llm-app-review-analysis/      # Repository root
â”‚
â”œâ”€â”€ app_reviews_pipeline/             # Main pipeline package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_config.py                # Multi-provider LLM configuration
â”‚   â”œâ”€â”€ preprocessing.py             # Data cleaning and preparation
â”‚   â”œâ”€â”€ prompt_optimize.py           # Automated prompt optimization
â”‚   â”œâ”€â”€ quality_judge.py             # LLM output quality evaluation
â”‚   â”œâ”€â”€ run_pipeline.py              # Main pipeline orchestration
â”‚   â”œâ”€â”€ user_selection.py            # Interactive CLI utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ M1_Discrepancy/              # Module 1: Statistical Analysis
â”‚   â”‚   â”œâ”€â”€ discrepancy.py           # VADER sentiment & discrepancy detection
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ M2_Absa_recommendation/      # Module 2: ABSA & Recommendations
â”‚   â”‚   â”œâ”€â”€ absa_recommendation.py   # Main ABSA pipeline
â”‚   â”‚   â”œâ”€â”€ absa_prompts.py          # ABSA-specific prompts
â”‚   â”‚   â”œâ”€â”€ absa_LLM_helpers.py      # ABSA utility functions
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ M3_Topic_modeling/           # Module 3: Topic Discovery
â”‚   â”‚   â”œâ”€â”€ topic_modeling.py        # BERTopic + LLM labeling
â”‚   â”‚   â”œâ”€â”€ topic_prompts.py         # Topic labeling prompts
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ M4_Rag_qa/                  # Module 4: Question Answering
â”‚       â”œâ”€â”€ rag_qa.py               # RAG-based Q&A system
â”‚       â”œâ”€â”€ rag_prompt.py           # RAG prompts and templates
â”‚       â”œâ”€â”€ rag_qus_samples.txt     # Sample questions
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                            # Data storage
â”‚   â”œâ”€â”€ raw/                        # Original datasets
â”‚   â”‚   â”œâ”€â”€ AWARE_Comprehensive.csv
â”‚   â”‚   â”œâ”€â”€ spotify_reviews.csv
â”‚   â”‚   â””â”€â”€ google_play_reviews.csv
â”‚   â””â”€â”€ processed/                  # Cleaned datasets
â”‚       â”œâ”€â”€ aware_clean.csv
â”‚       â”œâ”€â”€ spotify_clean.csv
â”‚       â”œâ”€â”€ google_play_clean.csv
â”‚       â””â”€â”€ *_stats.json            # Dataset statistics
â”‚
â”œâ”€â”€ outputs/                        # Analysis results
â”‚   â”œâ”€â”€ absa/                       # ABSA analysis outputs
â”‚   â”œâ”€â”€ discrepancy/                # Statistical analysis results
â”‚   â”œâ”€â”€ topic_modeling/             # Topic models and labels
â”‚   â”œâ”€â”€ rag_cache/                  # RAG embeddings and indices
â”‚   â””â”€â”€ prompt_dumps/               # Optimized prompts
â”‚
â”œâ”€â”€ additional_analysis/            # notebooks
â”‚   â”œâ”€â”€ ABSA_with_SOTA_model.ipynb
â”‚   â””â”€â”€ review_discrepancy_plots.ipynb
â”‚
â”œâ”€â”€ .env                            # Environment variables
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README                          # This file
```

## ğŸ® Usage Guide

### 1. ğŸ§¹ Data Preprocessing

First, clean and prepare your review dataset stored in data folder by app_reviews_pipeline/ running preprocessing.py



**Output:** Clean CSV files in `data/processed/` with statistics summaries.

**Interactive Flow:**
1. Choose dataset (AWARE, Spotify, Google Play)
2. Select LLM provider and model
3. Set sample size or use full dataset



### LLM Provider Setup

The framework supports multiple LLM providers through a unified interface:

```python
# llm_config.py
providers = {
    'openai': ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'],
    'mistral': ['mistral-large', 'mistral-medium'],
    'llama2': ['llama-2-70b', 'llama-2-13b']
}
```

### ğŸ¯ Research Impact
- **Academic Contribution**: Novel LLM framework surpassing fine-tuned transformer baselines
- **Practical Value**: Actionable insights for app developers beyond star ratings
- **Methodological Innovation**: Structured prompting approach with automated optimization
- **Scalable Solution**: Framework applicable across different app domains and review platforms



## ğŸ“š Citation

If you use this framework in your research, please cite our AICCSA 2025 paper:
