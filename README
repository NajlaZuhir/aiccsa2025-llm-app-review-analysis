# Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM

**Research Implementation for AICCSA 2025**

This repository contains the complete implementation of our research paper "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM" published in AICCSA 2025. Our framework addresses the fundamental limitations of traditional star-rating systems by developing an advanced LLM-based approach to mobile app review analysis that captures nuanced feedback often missed by numeric ratings alone.

## ðŸŽ¯ Research Objectives

Our research tackles the critical challenge that **star ratings fail to represent the nuanced feedback present in review texts**. While a 5-star rating appears positive, the accompanying text might reveal significant concerns about specific app features. Our LLM framework bridges this gap by:

- **Extracting Rich Insights**: Moving beyond surface-level ratings to understand detailed user experiences
- **Capturing Linguistic Nuances**: Handling sarcasm, domain-specific terminology, and contextual sentiment
- **Providing Actionable Feedback**: Delivering feature-level recommendations for app improvement
- **Enabling Interactive Analysis**: Supporting conversational exploration of review insights

## ðŸ”¬ Research Framework

Our modular LLM framework implements a three-component system validated on AWARE, Google Play, and Spotify datasets:

- **M1 - Statistical Baseline**: VADER sentiment analysis for comparison benchmarking  
- **M2 - ABSA with Recommendations**: Advanced aspect-sentiment extraction using structured prompting
- **M3 - LLM-Enhanced Topic Modeling**: BERTopic clustering with intelligent LLM-generated labels
- **M4 - RAG-Based Conversational QA**: Interactive question-answering over review corpus

## ðŸ“Š Research Contributions & Results

### ðŸ† **Performance Achievements**
- **5.1% F1-score improvement** in aspect extraction over fine-tuned DeBERTa-v3-large
- **Enhanced topic coherence**: Silhouette scores improved from -0.0313 to 0.0302 with LLM integration
- **Perfect retrieval diversity (100%)** with high relevance scores (cosine similarity: 0.618-0.754)
- **Quantified rating-sentiment gaps** across all datasets, revealing systematic discrepancies

### ðŸ”¬ **Technical Innovations**
- **Multi-LLM Architecture**: GPT-4, LLaMA 2 7B-chat, and Mistral 7B with unified interfaces
- **Automated Prompt Optimization**: Self-improving prompts with consistency checks and rule-based heuristics  
- **Structured Prompting**: Domain-aware prompt engineering for enhanced contextual understanding
- **Hybrid Approach**: Combines traditional NLP strengths with LLM contextual reasoning

## âœ¨ Key Features

### ðŸ”§ Multi-LLM Architecture
- **GPT-4/4o**: Primary model for complex reasoning and structured outputs
- **LLaMA 2 7B-chat**: Open-source alternative for accessibility and comparison
- **Mistral 7B**: High-performance model with efficient resource usage
- **Unified Interface**: Seamless switching between providers via standardized ChatFn interface

### ðŸ“Š Research-Validated Capabilities
- **Advanced ABSA**: Extract aspects, sentiments, and actionable recommendations using structured prompting
- **Intelligent Topic Discovery**: BERTopic clustering enhanced with LLM-generated labels and summaries
- **Interactive Review Analysis**: RAG-based conversational system for dynamic insight exploration
- **Automated Quality Assurance**: Self-evaluating prompts with consistency validation and optimization

### ðŸ› ï¸ Research-Grade Features
- **Multi-Dataset Validation**: Tested on AWARE, Google Play, and Spotify review corpora
- **Performance Benchmarking**: Systematic comparison against fine-tuned transformer models
- **Reproducible Results**: Deterministic outputs with comprehensive evaluation metrics
- **Scalable Architecture**: Handles datasets from samples to 80K+ reviews with intelligent caching

## ðŸš€ Quick Start

### Prerequisites
- Python 3.8+
- API keys for your chosen LLM provider(s)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/aiccsa2025-llm-app-review-analysis.git
cd aiccsa2025-llm-app-review-analysis
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables:**
Create a `.env` file in the root directory:
```bash
# Required - Choose your primary provider
OPENAI_API_KEY=your_openai_key_here

# Optional - Additional providers
MISTRAL_API_KEY=your_mistral_key_here
LLAMA_API_KEY=your_llama_key_here
```

## ðŸ“ Project Structure

```
aiccsa2025-llm-app-review-analysis/      # Repository root
â”‚
â”œâ”€â”€ app_reviews_pipeline/             # Main pipeline package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_config.py                # Multi-provider LLM configuration
â”‚   â”œâ”€â”€ preprocessing.py             # Data cleaning and preparation
â”‚   â”œâ”€â”€ prompt_optimize.py           # Automated prompt optimization
â”‚   â”œâ”€â”€ quality_judge.py             # LLM output quality evaluation
â”‚   â”œâ”€â”€ run_pipeline.py              # Main pipeline orchestration
â”‚   â”œâ”€â”€ user_selection.py            # Interactive CLI utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ M1_Discrepancy/              # Module 1: Statistical Analysis
â”‚   â”‚   â”œâ”€â”€ discrepancy.py           # VADER sentiment & discrepancy detection
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ M2_Absa_recommendation/      # Module 2: ABSA & Recommendations
â”‚   â”‚   â”œâ”€â”€ absa_recommendation.py   # Main ABSA pipeline
â”‚   â”‚   â”œâ”€â”€ absa_prompts.py          # ABSA-specific prompts
â”‚   â”‚   â”œâ”€â”€ absa_LLM_helpers.py      # ABSA utility functions
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ M3_Topic_modeling/           # Module 3: Topic Discovery
â”‚   â”‚   â”œâ”€â”€ topic_modeling.py        # BERTopic + LLM labeling
â”‚   â”‚   â”œâ”€â”€ topic_prompts.py         # Topic labeling prompts
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ M4_Rag_qa/                  # Module 4: Question Answering
â”‚       â”œâ”€â”€ rag_qa.py               # RAG-based Q&A system
â”‚       â”œâ”€â”€ rag_prompt.py           # RAG prompts and templates
â”‚       â”œâ”€â”€ rag_qus_samples.txt     # Sample questions
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                            # Data storage
â”‚   â”œâ”€â”€ raw/                        # Original datasets
â”‚   â”‚   â”œâ”€â”€ AWARE_Comprehensive.csv
â”‚   â”‚   â”œâ”€â”€ spotify_reviews.csv
â”‚   â”‚   â””â”€â”€ google_play_reviews.csv
â”‚   â””â”€â”€ processed/                  # Cleaned datasets
â”‚       â”œâ”€â”€ aware_clean.csv
â”‚       â”œâ”€â”€ spotify_clean.csv
â”‚       â”œâ”€â”€ google_play_clean.csv
â”‚       â””â”€â”€ *_stats.json            # Dataset statistics
â”‚
â”œâ”€â”€ outputs/                         # Analysis results
â”‚   â”œâ”€â”€ absa/                       # ABSA analysis outputs
â”‚   â”œâ”€â”€ discrepancy/                # Statistical analysis results
â”‚   â”œâ”€â”€ topic_modeling/             # Topic models and labels
â”‚   â”œâ”€â”€ rag_cache/                  # RAG embeddings and indices
â”‚   â””â”€â”€ prompt_dumps/               # Optimized prompts
â”‚
â”œâ”€â”€ other_analysis/                  # Additional notebooks
â”‚   â”œâ”€â”€ ABSA_with_SOTA_model.ipynb
â”‚   â””â”€â”€ review_discrepancy_plots.ipynb
â”‚
â”œâ”€â”€ .env                            # Environment variables
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README                          # This file
```

## ðŸŽ® Usage Guide

### 1. ðŸ§¹ Data Preprocessing

First, clean and prepare your review data:

```bash
cd app_reviews_pipeline
python preprocessing.py
```

**Interactive Options:**
- Select datasets: AWARE, Spotify, Google Play
- Choose processing parameters
- View cleaning statistics

**Output:** Clean CSV files in `data/processed/` with statistics summaries.

### 2. ðŸ“Š M1 - Discrepancy Analysis

Analyze sentiment discrepancies using VADER:

```bash
cd M1_Discrepancy
python discrepancy.py
```

**Features:**
- Statistical sentiment analysis
- Cross-dataset comparison
- Visualization generation

### 3. ðŸ’ M2 - ABSA Recommendation

Extract aspects, sentiments, and recommendations:

```bash
cd M2_Absa_recommendation
python absa_recommendation.py
```

**Interactive Flow:**
1. Choose dataset (AWARE, Spotify, Google Play)
2. Select LLM provider and model
3. Set sample size or use full dataset
4. Get structured ABSA output with quality scores

**Example Output:**
```json
{
  "review_id": "abc123",
  "aspects": ["app-stability", "user-interface"],
  "sentiments": {"app-stability": "Negative", "user-interface": "Positive"},
  "recommendations": ["Fix crash issues", "Maintain clean design"]
}
```

### 4. ðŸ·ï¸ M3 - Topic Modeling

Discover key themes with LLM-enhanced labeling:

```bash
cd M3_Topic_modeling
python topic_modeling.py
```

**Process:**
1. BERTopic clustering with optimized HDBSCAN parameters
2. LLM-generated topic labels and summaries
3. Quality assessment (coherence, distinctiveness)
4. Example reviews for each topic

**Output:** Comprehensive topic analysis with quality scores 6.4-8.6+

### 5. ðŸ¤– M4 - RAG Question Answering

Interactive Q&A over your review corpus:

```bash
cd M4_Rag_qa
python rag_qa.py
```

**Sample Questions:**
```
# Summary queries (triggers overview mode)
summary: overall themes in the reviews
overview of recurring complaints and praise

# Specific queries
what stability issues do users report most often?
what are the main frustrations with ads?
which features do users request most?

# Comparative analysis
summary: top 5 complaints (spotify dataset)
how do users feel about the latest update?
```

**Features:**
- Semantic search with FAISS indexing
- Evidence-based answers with review citations
- Actionable insights generation

## ðŸ”§ Advanced Configuration

### LLM Provider Setup

The framework supports multiple LLM providers through a unified interface:

```python
# llm_config.py
providers = {
    'openai': ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'],
    'mistral': ['mistral-large', 'mistral-medium'],
    'llama2': ['llama-2-70b', 'llama-2-13b']
}
```

### Quality Evaluation System

Automatic quality assessment for all LLM outputs:

- **ABSA Quality**: Aspect completeness, sentiment accuracy, recommendation actionability
- **Topic Quality**: Coherence scores, distinctiveness metrics, label clarity
- **Prompt Optimization**: Automated prompt tuning based on evaluation feedback

### Caching System

Intelligent caching for performance optimization:

- **Embeddings**: Sentence transformer vectors cached per dataset
- **Processed Data**: Clean datasets with incremental updates
- **Prompts**: Optimized prompts saved per model/dataset combination

## ðŸ“ˆ Performance & Scalability

### ðŸ“ˆ Research Validation & Benchmarks

| **Metric** | **Traditional Method** | **Our LLM Framework** | **Improvement** |
|------------|----------------------|----------------------|-----------------|
| Aspect Extraction F1 | DeBERTa-v3-large | GPT-4 + Structured Prompting | **+5.1%** |
| Topic Coherence | BERTopic alone (-0.0313) | LLM-Enhanced BERTopic (+0.0302) | **+0.0615** |
| QA Retrieval Diversity | Traditional RAG | Our RAG System | **100%** |
| Cosine Similarity | - | 0.618-0.754 | **High Relevance** |

### ðŸŽ¯ Research Impact
- **Academic Contribution**: Novel LLM framework surpassing fine-tuned transformer baselines
- **Practical Value**: Actionable insights for app developers beyond star ratings
- **Methodological Innovation**: Structured prompting approach with automated optimization
- **Scalable Solution**: Framework applicable across different app domains and review platforms

## ðŸ›¡ï¸ Quality Assurance

The framework includes comprehensive quality control:
### Automated Quality Checks

- **ABSA Output Validation**: Completeness and accuracy scoring
- **Topic Coherence Assessment**: Statistical and LLM-based evaluation
- **Recommendation Actionability**: Practical utility scoring
- **JSON Parsing Robustness**: Handles LLM output variations (markdown code blocks, etc.)

### Error Handling

- **Graceful Degradation**: System continues with warnings on non-critical failures
- **Retry Logic**: Automatic retry for transient API failures
- **Validation Pipeline**: Multi-stage validation of all outputs

## ðŸ¤ Contributing

We welcome contributions! Here's how to get started:

### Development Setup

1. **Fork and clone:**
```bash
git fork https://github.com/yourusername/aiccsa2025-llm-app-review-analysis.git
git clone your-fork-url
cd aiccsa2025-llm-app-review-analysis
```

2. **Create development environment:**
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
pip install -e .  # editable install
```

3. **Run tests:**
```bash
python -m pytest tests/
```

### Contribution Guidelines

- **Code Style**: Follow PEP 8, use black formatter
- **Documentation**: Update docstrings and README for new features
- **Testing**: Add tests for new functionality
- **Issues**: Check existing issues before creating new ones

### Areas for Contribution

- ðŸ”Œ **New LLM Providers**: Add support for additional LLM APIs
- ðŸ“Š **Visualization**: Enhanced plotting and dashboard features  
- ðŸŽ¯ **Domain Adaptation**: Specialized prompts for different app categories
- âš¡ **Performance**: Optimization for larger datasets
- ðŸ§ª **Evaluation Metrics**: New quality assessment methods

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ“š Citation

If you use this framework in your research, please cite our AICCSA 2025 paper:

```bibtex
@inproceedings{zuhir2025beyond,
  author = {Najla Zuhir and Amna Mohammad Salim and Parvathy Premkumar and Md Moshiur Farazi},
  title = {Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM},
  booktitle = {Proceedings of the IEEE/ACS International Conference on Computer Systems and Applications (AICCSA)},
  year = {2025},
  organization = {IEEE},
  institution = {University of Doha for Science and Technology, College of Computing \& Information Technology},
  url = {https://github.com/yourusername/app-reviews-llm-analysis-main}
}
```

## ðŸ‘¥ Research Team

**University of Doha for Science and Technology**  
*College of Computing & Information Technology*

- **Najla Zuhir** - Lead Researcher
- **Amna Mohammad Salim** - Co-Researcher  
- **Parvathy Premkumar** - Co-Researcher
- **Md Moshiur Farazi** - Co-Researcher

## ðŸ™ Acknowledgments

### Core Technologies
- **[BERTopic](https://github.com/MaartenGr/BERTopic)**: Advanced topic modeling capabilities
- **[FAISS](https://github.com/facebookresearch/faiss)**: Efficient similarity search and clustering
- **[Sentence Transformers](https://www.sbert.net/)**: State-of-the-art sentence embeddings
- **[VADER](https://github.com/cjhutto/vaderSentiment)**: Lexicon-based sentiment analysis

### LLM Providers
- **OpenAI**: GPT-4o and GPT-4o-mini API access
- **Mistral AI**: High-quality open-source model alternatives
- **Meta**: LLaMA 2 models for research applications

### Research Community
- **AICCSA 2025** - IEEE/ACS International Conference on Computer Systems and Applications
- **University of Doha for Science and Technology** - Institutional support and resources
- Thanks to all contributors and researchers advancing computational linguistics and sentiment analysis

---

## ðŸ—ï¸ Research Architecture

```mermaid
flowchart TD
    A[App Review Corpus] --> B[Multi-Dataset Preprocessing]
    B --> C{LLM Framework Selection}
    C -->|Baseline| D[M1: VADER Analysis]
    C -->|Research Focus| E[M2: ABSA + Recommendations]
    C -->|Innovation| F[M3: LLM-Enhanced Topic Modeling]
    C -->|Application| G[M4: RAG-Based QA]
    H[Multi-LLM Providers<br/>GPT-4, LLaMA 2, Mistral] --> E
    H --> F
    H --> G
    I[Automated Quality Evaluation] --> E
    I --> F
    J[Structured Prompt Optimization] --> E
    J --> F
    J --> G
    K[Performance Benchmarking] --> L[Research Validation]
    E --> K
    F --> K
    G --> K
```

**Ready to explore beyond star ratings? Start with our [Quick Start](#-quick-start) guide and discover the hidden insights in app reviews!** ðŸš€
